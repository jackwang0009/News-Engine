#!/usr/bin/env python3
"""
æŸ¥çœ‹æ–°é—»æºä¿¡æ¯çš„è„šæœ¬
"""
import requests
import json
from datetime import datetime

def check_sources():
    """æŸ¥çœ‹æ‰€æœ‰æ–°é—»æº"""
    try:
        response = requests.get('http://localhost:9000/api/v1/sources/')
        if response.status_code == 200:
            sources = response.json()
            print(f"ğŸ“° æ‰¾åˆ° {len(sources)} ä¸ªæ–°é—»æº:")
            print("=" * 60)
            
            for i, source in enumerate(sources, 1):
                print(f"\n{i}. æ–°é—»æºID: {source.get('id', 'N/A')}")
                print(f"   åç§°: {source.get('name', 'N/A')}")
                print(f"   URL: {source.get('url', 'N/A')}")
                print(f"   ç±»å‹: {source.get('type', 'N/A')}")
                print(f"   è§£æå™¨: {source.get('parser', 'N/A')}")
                print(f"   çŠ¶æ€: {'âœ… æ´»è·ƒ' if source.get('is_active') else 'âŒ åœç”¨'}")
                print(f"   çˆ¬å–é—´éš”: {source.get('crawl_interval', 'N/A')} ç§’")
                print(f"   æœ€åçˆ¬å–: {source.get('last_crawl_time', 'ä»æœªçˆ¬å–')}")
                print(f"   åˆ›å»ºæ—¶é—´: {source.get('created_at', 'N/A')}")
                
        else:
            print(f"âŒ è·å–æ–°é—»æºå¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

def check_crawler_status():
    """æŸ¥çœ‹çˆ¬è™«çŠ¶æ€"""
    try:
        response = requests.get('http://localhost:9000/api/v1/crawlers/status')
        if response.status_code == 200:
            status = response.json()
            print("\nğŸ“Š çˆ¬è™«çŠ¶æ€:")
            print("=" * 60)
            print(f"æ€»æ–°é—»æºæ•°é‡: {status.get('total_sources', 0)}")
            print(f"æ´»è·ƒæ–°é—»æº: {status.get('active_sources', 0)}")
            print(f"è¿è¡Œä¸­ä»»åŠ¡: {status.get('running_tasks', 0)}")
            print(f"é˜Ÿåˆ—ä¸­ä»»åŠ¡: {status.get('queued_tasks', 0)}")
            print(f"ä»Šæ—¥å®Œæˆ: {status.get('completed_today', 0)} æ¡")
            print(f"ä»Šæ—¥å¤±è´¥: {status.get('failed_today', 0)} æ¡")
            print(f"æœ€åçˆ¬å–æ—¶é—´: {status.get('last_crawl_time', 'æœªçŸ¥')}")
            print(f"æ•´ä½“çŠ¶æ€: {status.get('overall_status', 'æœªçŸ¥')}")
        else:
            print(f"âŒ è·å–çˆ¬è™«çŠ¶æ€å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

def check_recent_news():
    """æŸ¥çœ‹æœ€è¿‘çš„æ–°é—»æ•°æ®"""
    try:
        response = requests.get('http://localhost:9000/api/v1/news/')
        if response.status_code == 200:
            news_data = response.json()
            print(f"\nğŸ“° æœ€è¿‘æ–°é—»æ•°æ®:")
            print("=" * 80)
            print(f"ğŸ“Š æ€»æ–°é—»æ•°é‡: {len(news_data) if isinstance(news_data, list) else 'éåˆ—è¡¨'}")
            
            # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
            if isinstance(news_data, dict):
                print(f"ğŸ” æ–°é—»å“åº”å­—æ®µ: {list(news_data.keys())}")
                if 'data' in news_data and 'items' in news_data['data']:
                    news_list = news_data['data']['items']
                elif 'articles' in news_data:
                    news_list = news_data['articles']
                else:
                    news_list = []
            else:
                news_list = news_data if isinstance(news_data, list) else []
            
            if news_list:
                # æ˜¾ç¤ºæœ€æ–°çš„å‡ æ¡æ–°é—»
                recent_news = news_list[:3]
                for i, news in enumerate(recent_news, 1):
                    print(f"\n{i}. ğŸ“° {news.get('title', 'æ— æ ‡é¢˜')}")
                    print(f"    ğŸ“ æ¥æº: {news.get('source', 'æœªçŸ¥')}")
                    print(f"    ğŸŒ URL: {news.get('url', 'æ— é“¾æ¥')}")
                    print(f"    ğŸ“… å‘å¸ƒæ—¶é—´: {news.get('published_at', 'æœªçŸ¥')}")
                    print(f"    ğŸ“ æ‘˜è¦: {news.get('summary', 'æ— æ‘˜è¦')[:100] if news.get('summary') else 'æ— æ‘˜è¦'}...")
            else:
                print("    ğŸ“­ æš‚æ— æ–°é—»æ•°æ®")
                
        else:
            print(f"âŒ è·å–æ–°é—»æ•°æ®å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯è¯¦æƒ…: {response.text}")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_api_directly():
    """ç›´æ¥æµ‹è¯•APIï¼Œæ˜¾ç¤ºåŸå§‹å“åº”"""
    print("\nğŸ§ª ç›´æ¥æµ‹è¯•APIå“åº”:")
    print("=" * 80)
    
    # æµ‹è¯•æ–°é—»æºAPI
    print("ğŸ“¡ æµ‹è¯• /api/v1/sources/ æ¥å£:")
    try:
        response = requests.get('http://localhost:9000/api/v1/sources/')
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {dict(response.headers)}")
        print(f"å“åº”å†…å®¹: {response.text[:500]}...")
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
    
    # æµ‹è¯•çˆ¬è™«çŠ¶æ€API
    print("\nğŸ“¡ æµ‹è¯• /api/v1/crawlers/status æ¥å£:")
    try:
        response = requests.get('http://localhost:9000/api/v1/crawlers/status')
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å†…å®¹: {response.text[:500]}...")
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ–°é—»æºä¿¡æ¯æŸ¥çœ‹å·¥å…·")
    print("=" * 80)
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ç›´æ¥æµ‹è¯•API
    test_api_directly()
    
    # æŸ¥çœ‹çˆ¬è™«çŠ¶æ€
    check_crawler_status()
    
    # æŸ¥çœ‹æ–°é—»æºè¯¦æƒ…
    check_sources()
    
    # æŸ¥çœ‹æœ€è¿‘çš„æ–°é—»æ•°æ®
    check_recent_news()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ£€æŸ¥å®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("   - æ–°æ·»åŠ çš„æ–°é—»æºä¼šæ˜¾ç¤º ğŸ†• æ ‡è®°")
    print("   - æ–°é—»æºæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢")
    print("   - å¯ä»¥è¿è¡Œ 'python load_news_sources.py' æ·»åŠ æ›´å¤šæ–°é—»æº")

if __name__ == "__main__":
    main()
