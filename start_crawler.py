#!/usr/bin/env python3
"""
å¯åŠ¨çˆ¬è™«ç³»ç»Ÿè„šæœ¬
ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ–°é—»æºè¿›è¡Œçˆ¬å–
"""
import json
import requests
import time
import sys
from datetime import datetime
from typing import Dict, Any, List

class CrawlerStarter:
    """çˆ¬è™«å¯åŠ¨å™¨"""
    
    def __init__(self):
        self.api_base_url = 'http://localhost:9000/api/v1'
        self.news_sources = self.load_news_sources()
        
    def load_news_sources(self) -> Dict[str, Any]:
        """åŠ è½½æ–°é—»æºé…ç½®"""
        try:
            with open('news_sources.json', 'r', encoding='utf-8') as f:
                sources = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½æ–°é—»æºé…ç½®: {len(sources)} ä¸ªåˆ†ç±»")
            return sources
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ° news_sources.json é…ç½®æ–‡ä»¶")
            return {}
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            return {}
    
    def add_news_sources(self) -> List[Dict[str, Any]]:
        """æ·»åŠ æ–°é—»æºåˆ°ç³»ç»Ÿ"""
        print("\nğŸš€ æ·»åŠ æ–°é—»æºåˆ°ç³»ç»Ÿ")
        print("=" * 60)
        
        all_sources = []
        for category, sources in self.news_sources.items():
            if isinstance(sources, list):
                all_sources.extend(sources)
        
        print(f"ğŸ“Š æ€»è®¡: {len(all_sources)} ä¸ªæ–°é—»æº")
        
        added_sources = []
        failed_sources = []
        
        for i, source in enumerate(all_sources, 1):
            print(f"\nğŸ“ å¤„ç†ç¬¬ {i}/{len(all_sources)} ä¸ªæ–°é—»æº:")
            print(f"   ğŸ“° åç§°: {source.get('name', 'N/A')}")
            print(f"   ğŸŒ URL: {source.get('url', 'N/A')}")
            print(f"   ğŸ·ï¸ ç±»å‹: {source.get('type', 'N/A')}")
            
            try:
                source_data = {
                    "name": source.get('name', ''),
                    "url": source.get('url', ''),
                    "type": source.get('type', 'website'),
                    "parser": self._generate_parser_name(source),
                    "crawl_interval": self._get_crawl_interval(source),
                    "is_active": True
                }
                
                if source.get('notes'):
                    source_data['description'] = source.get('notes')
                
                response = requests.post(
                    f'{self.api_base_url}/sources/',
                    json=source_data,
                    headers={'Content-Type': 'application/json'},
                    timeout=10
                )
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"   âœ… æ·»åŠ æˆåŠŸ! ID: {result.get('id', 'N/A')}")
                    added_sources.append({
                        'source': source,
                        'result': result
                    })
                else:
                    print(f"   âŒ æ·»åŠ å¤±è´¥: {response.status_code}")
                    print(f"   é”™è¯¯è¯¦æƒ…: {response.text}")
                    failed_sources.append(source)
                    
            except Exception as e:
                print(f"   âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
                failed_sources.append(source)
            
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ·»åŠ ç»“æœç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸæ·»åŠ : {len(added_sources)} ä¸ª")
        print(f"âŒ æ·»åŠ å¤±è´¥: {len(failed_sources)} ä¸ª")
        
        return added_sources, failed_sources
    
    def _generate_parser_name(self, source: Dict[str, Any]) -> str:
        """æ ¹æ®æ–°é—»æºç”Ÿæˆè§£æå™¨åç§°"""
        name = source.get('name', '').lower()
        
        if 'baidu' in name:
            return 'baidu'
        elif 'sina' in name:
            return 'sina'
        elif 'sohu' in name:
            return 'sohu'
        elif 'qq' in name or 'tencent' in name:
            return 'tencent'
        elif 'sogou' in name:
            return 'sogou'
        elif 'toutiao' in name:
            return 'toutiao'
        elif 'yidian' in name:
            return 'yidian'
        else:
            return 'generic_website'
    
    def _get_crawl_interval(self, source: Dict[str, Any]) -> int:
        """æ ¹æ®æ–°é—»æºç±»å‹è·å–çˆ¬å–é—´éš”"""
        # æ ¹æ®éš¾åº¦åˆ†ç±»è®¾ç½®ä¸åŒçš„çˆ¬å–é—´éš”
        source_name = source.get('name', '').lower()
        
        if any(keyword in source_name for keyword in ['baidu', 'sina', 'sohu']):
            return 300  # ç®€å•æºï¼š5åˆ†é’Ÿ
        elif any(keyword in source_name for keyword in ['qq', 'tencent', 'sogou']):
            return 600  # ä¸­ç­‰æºï¼š10åˆ†é’Ÿ
        else:
            return 900  # å›°éš¾æºï¼š15åˆ†é’Ÿ
    
    def start_crawler_tasks(self, sources: List[Dict[str, Any]]):
        """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
        print("\nğŸ•·ï¸ å¯åŠ¨çˆ¬è™«ä»»åŠ¡")
        print("=" * 60)
        
        if not sources:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–°é—»æº")
            return
        
        print(f"ğŸ“Š å‡†å¤‡å¯åŠ¨ {len(sources)} ä¸ªçˆ¬è™«ä»»åŠ¡")
        
        # é€ä¸ªå¯åŠ¨çˆ¬è™«ä»»åŠ¡
        started_tasks = []
        failed_tasks = []
        
        for source in sources:
            source_id = source['result']['id']
            source_name = source['source']['name']
            
            print(f"\nğŸ“ å¯åŠ¨çˆ¬è™«ä»»åŠ¡:")
            print(f"   ğŸ“° æ–°é—»æº: {source_name}")
            print(f"   ğŸ†” ID: {source_id}")
            
            try:
                # å¯åŠ¨å•ä¸ªçˆ¬è™«ä»»åŠ¡
                response = requests.post(
                    f'{self.api_base_url}/crawlers/start',
                    json={
                        "source_id": source_id,
                        "force_crawl": False,
                        "max_pages": 5
                    },
                    timeout=10
                )
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"   âœ… å¯åŠ¨æˆåŠŸ!")
                    
                    # ä»å“åº”ä¸­æå–ä»»åŠ¡ä¿¡æ¯
                    task_info = result.get('task', {})
                    task_id = task_info.get('task_id', 'N/A')
                    status = task_info.get('status', 'N/A')
                    source_id = task_info.get('source_id', 'N/A')
                    created_at = task_info.get('created_at', 'N/A')
                    
                    print(f"   ä»»åŠ¡ID: {task_id}")
                    print(f"   çŠ¶æ€: {status}")
                    print(f"   æ–°é—»æºID: {source_id}")
                    print(f"   åˆ›å»ºæ—¶é—´: {created_at}")
                    
                    started_tasks.append({
                        'source_name': source_name,
                        'source_id': source_id,
                        'task_id': task_id,
                        'status': status,
                        'result': result
                    })
                else:
                    print(f"   âŒ å¯åŠ¨å¤±è´¥: {response.status_code}")
                    print(f"   é”™è¯¯è¯¦æƒ…: {response.text}")
                    failed_tasks.append({
                        'source_name': source_name,
                        'source_id': source_id,
                        'error': response.text
                    })
                    
            except Exception as e:
                print(f"   âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
                failed_tasks.append({
                    'source_name': source_name,
                    'source_id': source_id,
                    'error': str(e)
                })
            
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬è™«ä»»åŠ¡å¯åŠ¨ç»“æœ:")
        print(f"âœ… æˆåŠŸå¯åŠ¨: {len(started_tasks)} ä¸ª")
        print(f"âŒ å¯åŠ¨å¤±è´¥: {len(failed_tasks)} ä¸ª")
        
        if failed_tasks:
            print("\nâŒ å¯åŠ¨å¤±è´¥çš„æ–°é—»æº:")
            for task in failed_tasks:
                print(f"   - {task['source_name']} (ID: {task['source_id']}): {task['error']}")
        
        return started_tasks, failed_tasks
    
    def check_system_status(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"""
        print("\nğŸ” æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        print("=" * 60)
        
        # æ£€æŸ¥APIå¥åº·çŠ¶æ€
        try:
            response = requests.get(f'{self.api_base_url}/health')
            if response.status_code == 200:
                health = response.json()
                print(f"âœ… APIæœåŠ¡çŠ¶æ€: {health.get('status', 'N/A')}")
            else:
                print(f"âŒ APIæœåŠ¡å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥APIæœåŠ¡: {str(e)}")
            return False
        
        # æ£€æŸ¥çˆ¬è™«çŠ¶æ€
        try:
            response = requests.get(f'{self.api_base_url}/crawlers/status')
            if response.status_code == 200:
                status = response.json()
                print(f"ğŸ“Š çˆ¬è™«ç³»ç»ŸçŠ¶æ€: {status.get('overall_status', 'N/A')}")
                print(f"   æ€»æ–°é—»æº: {status.get('total_sources', 0)}")
                print(f"   æ´»è·ƒæº: {status.get('active_sources', 0)}")
            else:
                print(f"âŒ æ— æ³•è·å–çˆ¬è™«çŠ¶æ€: {response.status_code}")
        except Exception as e:
            print(f"âŒ æ£€æŸ¥çˆ¬è™«çŠ¶æ€å¤±è´¥: {str(e)}")
        
        return True
    
    def check_started_tasks(self, started_tasks: List[Dict[str, Any]]):
        """æ£€æŸ¥å·²å¯åŠ¨ä»»åŠ¡çš„çŠ¶æ€"""
        if not started_tasks:
            return
        
        print("\nğŸ” æ£€æŸ¥å·²å¯åŠ¨ä»»åŠ¡çš„çŠ¶æ€:")
        print("=" * 60)
        
        for task in started_tasks:
            task_id = task.get('task_id')
            source_name = task.get('source_name')
            source_id = task.get('source_id')
            
            if not task_id or task_id == 'N/A':
                continue
                
            print(f"\nğŸ“ ä»»åŠ¡çŠ¶æ€æ£€æŸ¥:")
            print(f"   ğŸ“° æ–°é—»æº: {source_name}")
            print(f"   ğŸ†” ä»»åŠ¡ID: {task_id}")
            print(f"   ğŸ“ æ–°é—»æºID: {source_id}")
            
            try:
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                response = requests.get(
                    f'{self.api_base_url}/crawlers/{task_id}/status',
                    timeout=10
                )
                
                if response.status_code == 200:
                    status_data = response.json()
                    print(f"   ğŸ“Š çŠ¶æ€: {status_data.get('status', 'N/A')}")
                    print(f"   ğŸ“ˆ è¿›åº¦: {status_data.get('progress', 'N/A')}%")
                    print(f"   ğŸ“° å·²æ‰¾åˆ°æ–‡ç« : {status_data.get('articles_found', 'N/A')}")
                    print(f"   âœ… å·²å¤„ç†æ–‡ç« : {status_data.get('articles_processed', 'N/A')}")
                else:
                    print(f"   âŒ çŠ¶æ€æ£€æŸ¥å¤±è´¥: {response.status_code}")
                    
            except Exception as e:
                print(f"   âŒ çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {str(e)}")
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

    def run(self):
        """è¿è¡Œçˆ¬è™«å¯åŠ¨å™¨"""
        print("ğŸ•·ï¸ æ–°é—»çˆ¬è™«ç³»ç»Ÿå¯åŠ¨å™¨")
        print("=" * 60)
        print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
        if not self.check_system_status():
            print("âŒ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£å¸¸è¿è¡Œ")
            return
        
        # æ·»åŠ æ–°é—»æº
        added_sources, failed_sources = self.add_news_sources()
        
        if not added_sources:
            print("âŒ æ²¡æœ‰æˆåŠŸæ·»åŠ ä»»ä½•æ–°é—»æºï¼Œæ— æ³•å¯åŠ¨çˆ¬è™«")
            return
        
        # å¯åŠ¨çˆ¬è™«ä»»åŠ¡
        started_tasks, failed_tasks = self.start_crawler_tasks(added_sources)
        
        # æ£€æŸ¥å·²å¯åŠ¨ä»»åŠ¡çš„çŠ¶æ€
        self.check_started_tasks(started_tasks)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬è™«ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("   1. æ£€æŸ¥ Celery Worker æ˜¯å¦è¿è¡Œ: python scripts/start_celery_worker.py")
        print("   2. æ£€æŸ¥ Celery Beat æ˜¯å¦è¿è¡Œ: python scripts/start_celery_beat.py")
        print("   3. ç›‘æ§çˆ¬è™«çŠ¶æ€: python check_sources.py")
        print("   4. æŸ¥çœ‹çˆ¬è™«ç›‘æ§: http://localhost:5555")

def main():
    """ä¸»å‡½æ•°"""
    starter = CrawlerStarter()
    starter.run()

if __name__ == "__main__":
    main()
