#!/usr/bin/env python3
"""
æ·»åŠ æ–°æ–°é—»æºçš„ç¤ºä¾‹è„šæœ¬
"""
import requests
import json
from datetime import datetime

def add_news_source():
    """æ·»åŠ æ–°çš„æ–°é—»æº"""
    
    # æ–°æ–°é—»æºé…ç½®
    new_source = {
        "name": "ç½‘æ˜“æ–°é—»",
        "url": "https://news.163.com",
        "type": "website",
        "parser": "netease",
        "crawl_interval": 300  # 5åˆ†é’Ÿçˆ¬å–ä¸€æ¬¡
    }
    
    try:
        print("ğŸš€ æ­£åœ¨æ·»åŠ æ–°æ–°é—»æº...")
        print(f"ğŸ“° åç§°: {new_source['name']}")
        print(f"ğŸŒ URL: {new_source['url']}")
        print(f"ğŸ”§ ç±»å‹: {new_source['type']}")
        print(f"âš™ï¸ è§£æå™¨: {new_source['parser']}")
        print(f"â° çˆ¬å–é—´éš”: {new_source['crawl_interval']} ç§’")
        print("=" * 60)
        
        # å‘é€POSTè¯·æ±‚åˆ›å»ºæ–°é—»æº
        response = requests.post(
            'http://localhost:9000/api/v1/sources/',
            json=new_source,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… æ–°é—»æºåˆ›å»ºæˆåŠŸï¼")
            print(f"ğŸ†” æ–°é—»æºID: {result.get('id', 'N/A')}")
            print(f"ğŸ“… åˆ›å»ºæ—¶é—´: {result.get('created_at', 'N/A')}")
            print(f"ğŸ”„ æ›´æ–°æ—¶é—´: {result.get('updated_at', 'N/A')}")
            
            return result
            
        else:
            print(f"âŒ åˆ›å»ºå¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return None
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None

def list_all_sources():
    """åˆ—å‡ºæ‰€æœ‰æ–°é—»æº"""
    try:
        print("\nğŸ“‹ å½“å‰æ‰€æœ‰æ–°é—»æº:")
        print("=" * 60)
        
        response = requests.get('http://localhost:9000/api/v1/sources/')
        if response.status_code == 200:
            sources = response.json()
            
            for i, source in enumerate(sources, 1):
                print(f"\n{i}. æ–°é—»æºID: {source.get('id', 'N/A')}")
                print(f"   åç§°: {source.get('name', 'N/A')}")
                print(f"   URL: {source.get('url', 'N/A')}")
                print(f"   ç±»å‹: {source.get('type', 'N/A')}")
                print(f"   è§£æå™¨: {source.get('parser', 'N/A')}")
                print(f"   çŠ¶æ€: {'âœ… æ´»è·ƒ' if source.get('is_active') else 'âŒ åœç”¨'}")
                print(f"   çˆ¬å–é—´éš”: {source.get('crawl_interval', 'N/A')} ç§’")
                print(f"   æœ€åçˆ¬å–: {source.get('last_crawl_time', 'ä»æœªçˆ¬å–')}")
                
        else:
            print(f"âŒ è·å–æ–°é—»æºåˆ—è¡¨å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

def test_crawler_for_source(source_id: str):
    """æµ‹è¯•ç‰¹å®šæ–°é—»æºçš„çˆ¬è™«"""
    try:
        print(f"\nğŸ§ª æµ‹è¯•æ–°é—»æº {source_id} çš„çˆ¬è™«...")
        print("=" * 60)
        
        # å¯åŠ¨çˆ¬è™«ä»»åŠ¡
        task_data = {
            "source_id": source_id,
            "force_crawl": True,
            "max_pages": 3
        }
        
        response = requests.post(
            'http://localhost:9000/api/v1/crawlers/start',
            json=task_data,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… çˆ¬è™«ä»»åŠ¡å¯åŠ¨æˆåŠŸï¼")
            print(f"ğŸ†” ä»»åŠ¡ID: {result.get('task', {}).get('task_id', 'N/A')}")
            print(f"ğŸ“° æ–°é—»æº: {result.get('task', {}).get('source_id', 'N/A')}")
            print(f"ğŸ“Š çŠ¶æ€: {result.get('task', {}).get('status', 'N/A')}")
            print(f"â° åˆ›å»ºæ—¶é—´: {result.get('task', {}).get('created_at', 'N/A')}")
            
        else:
            print(f"âŒ å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ†• æ–°é—»æºæ·»åŠ å·¥å…·")
    print("=" * 60)
    
    # 1. æ·»åŠ æ–°æ–°é—»æº
    new_source = add_news_source()
    
    if new_source:
        # 2. åˆ—å‡ºæ‰€æœ‰æ–°é—»æº
        list_all_sources()
        
        # 3. æµ‹è¯•æ–°æ–°é—»æºçš„çˆ¬è™«
        source_id = new_source.get('id')
        if source_id:
            test_crawler_for_source(source_id)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ–°é—»æºæ·»åŠ å®Œæˆï¼")
        print("\nğŸ’¡ åç»­æ­¥éª¤:")
        print("1. ç¡®ä¿ Celery Worker æ­£åœ¨è¿è¡Œ")
        print("2. å¯åŠ¨ Celery Beat ä»¥å¯ç”¨å®šæ—¶çˆ¬å–")
        print("3. ç›‘æ§çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€")
        print("4. æŸ¥çœ‹çˆ¬å–åˆ°çš„æ–°é—»æ•°æ®")
        
    else:
        print("\nâŒ æ–°é—»æºæ·»åŠ å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print("1. APIæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. è¯·æ±‚å‚æ•°æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()
