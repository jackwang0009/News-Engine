#!/usr/bin/env python3
"""
æ™ºèƒ½æ–°é—»æºå‘ç°å’Œé…ç½®å·¥å…·
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import json
from typing import List, Dict, Any, Optional
import time

class SmartSourceDiscovery:
    """æ™ºèƒ½æ–°é—»æºå‘ç°å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def discover_news_sources(self, seed_url: str) -> List[Dict[str, Any]]:
        """æ™ºèƒ½å‘ç°æ–°é—»æº"""
        print(f"ğŸ” æ­£åœ¨åˆ†æç½‘ç«™: {seed_url}")
        print("=" * 60)
        
        discovered_sources = []
        
        try:
            # 1. è·å–ä¸»é¡µå†…å®¹
            response = self.session.get(seed_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 2. è‡ªåŠ¨è¯†åˆ«ç½‘ç«™ç±»å‹
            site_type = self.identify_site_type(soup, seed_url)
            print(f"ğŸ·ï¸ è¯†åˆ«ç½‘ç«™ç±»å‹: {site_type}")
            
            # 3. æŸ¥æ‰¾RSSæº
            rss_sources = self.find_rss_feeds(soup, seed_url)
            if rss_sources:
                discovered_sources.extend(rss_sources)
                print(f"ğŸ“¡ å‘ç° {len(rss_sources)} ä¸ªRSSæº")
            
            # 4. åˆ†æç½‘ç«™ç»“æ„
            if site_type == "news_site":
                website_sources = self.analyze_news_structure(soup, seed_url)
                if website_sources:
                    discovered_sources.extend(website_sources)
                    print(f"ğŸŒ å‘ç° {len(website_sources)} ä¸ªç½‘ç«™çˆ¬è™«æº")
            
            # 5. æ™ºèƒ½ç”Ÿæˆé…ç½®
            for source in discovered_sources:
                source['config'] = self.generate_crawler_config(source)
            
            return discovered_sources
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            return []
    
    def identify_site_type(self, soup: BeautifulSoup, url: str) -> str:
        """è¯†åˆ«ç½‘ç«™ç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–°é—»ç‰¹å¾
        news_indicators = [
            'news', 'æ–°é—»', 'èµ„è®¯', 'æŠ¥é“', 'å¤´æ¡',
            'article', 'story', 'post', 'blog'
        ]
        
        text_content = soup.get_text().lower()
        title = soup.title.get_text().lower() if soup.title else ""
        
        # æ£€æŸ¥é¡µé¢ç»“æ„
        has_news_structure = any([
            soup.select('.news-list'),
            soup.select('.article-list'),
            soup.select('.post-list'),
            soup.select('[class*="news"]'),
            soup.select('[class*="article"]')
        ])
        
        if any(indicator in text_content for indicator in news_indicators) or has_news_structure:
            return "news_site"
        elif "rss" in text_content or "feed" in text_content:
            return "rss_site"
        else:
            return "general_site"
    
    def find_rss_feeds(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾RSSè®¢é˜…æº"""
        rss_sources = []
        
        # 1. æŸ¥æ‰¾RSSé“¾æ¥æ ‡ç­¾
        rss_links = soup.find_all('link', {
            'type': ['application/rss+xml', 'application/atom+xml', 'text/xml']
        })
        
        for link in rss_links:
            href = link.get('href')
            if href:
                rss_url = urljoin(base_url, href)
                title = link.get('title', 'RSSè®¢é˜…')
                
                rss_sources.append({
                    'name': f"{title} - RSS",
                    'url': rss_url,
                    'type': 'rss',
                    'parser': 'rss',
                    'crawl_interval': 600,  # 10åˆ†é’Ÿ
                    'discovery_method': 'rss_link_tag'
                })
        
        # 2. æ£€æŸ¥å¸¸è§RSSè·¯å¾„
        common_rss_paths = [
            '/rss.xml', '/feed.xml', '/atom.xml',
            '/news/rss', '/feed', '/rss',
            '/sitemap.xml', '/sitemap_news.xml'
        ]
        
        for path in common_rss_paths:
            try:
                rss_url = urljoin(base_url, path)
                response = self.session.head(rss_url, timeout=5)
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '')
                    if 'xml' in content_type or 'rss' in content_type:
                        rss_sources.append({
                            'name': f"RSSæº - {path}",
                            'url': rss_url,
                            'type': 'rss',
                            'parser': 'rss',
                            'crawl_interval': 600,
                            'discovery_method': 'common_path'
                        })
            except:
                continue
        
        return rss_sources
    
    def analyze_news_structure(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """åˆ†ææ–°é—»ç½‘ç«™ç»“æ„"""
        website_sources = []
        
        # 1. æŸ¥æ‰¾æ–°é—»åˆ—è¡¨é¡µ
        news_list_patterns = [
            'news', 'èµ„è®¯', 'æŠ¥é“', 'å¤´æ¡', 'è¦é—»',
            'article', 'story', 'post', 'blog'
        ]
        
        # æŸ¥æ‰¾å¯¼èˆªé“¾æ¥
        nav_links = soup.find_all('a', href=True)
        news_sections = []
        
        for link in nav_links:
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é—»ç›¸å…³é“¾æ¥
            if any(pattern in text for pattern in news_list_patterns):
                news_url = urljoin(base_url, href)
                news_sections.append({
                    'name': link.get_text(strip=True),
                    'url': news_url,
                    'text': text
                })
        
        # 2. ç”Ÿæˆç½‘ç«™çˆ¬è™«é…ç½®
        if news_sections:
            # é€‰æ‹©ä¸»è¦çš„æ–°é—»ç‰ˆå—
            main_news_section = max(news_sections, 
                                  key=lambda x: len([p for p in news_list_patterns if p in x['text']]))
            
            website_sources.append({
                'name': f"{main_news_section['name']} - ç½‘ç«™çˆ¬è™«",
                'url': main_news_section['url'],
                'type': 'website',
                'parser': 'auto_generated',
                'crawl_interval': 300,  # 5åˆ†é’Ÿ
                'discovery_method': 'structure_analysis',
                'section_name': main_news_section['name']
            })
        
        return website_sources
    
    def generate_crawler_config(self, source: Dict[str, Any]) -> Dict[str, Any]:
        """æ™ºèƒ½ç”Ÿæˆçˆ¬è™«é…ç½®"""
        config = {
            'basic': {
                'delay': 1.0,
                'timeout': 30,
                'max_retries': 3,
                'max_pages': 10
            }
        }
        
        if source['type'] == 'rss':
            config['rss'] = {
                'parse_items': True,
                'parse_content': True,
                'max_items': 50
            }
        elif source['type'] == 'website':
            config['website'] = {
                'list_page_selectors': self.suggest_selectors(source['url']),
                'article_selectors': self.suggest_article_selectors(),
                'pagination_selectors': self.suggest_pagination_selectors()
            }
        
        return config
    
    def suggest_selectors(self, url: str) -> Dict[str, str]:
        """å»ºè®®CSSé€‰æ‹©å™¨"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ™ºèƒ½è¯†åˆ«é€‰æ‹©å™¨
            selectors = {}
            
            # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨å®¹å™¨
            list_containers = soup.select('[class*="list"], [class*="news"], [class*="article"]')
            if list_containers:
                selectors['container'] = self.generate_css_selector(list_containers[0])
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            article_links = soup.find_all('a', href=True)
            if article_links:
                # åˆ†æé“¾æ¥æ¨¡å¼
                href_patterns = [link.get('href') for link in article_links[:10]]
                selectors['article_link'] = 'a[href*="news"], a[href*="article"]'
            
            return selectors
            
        except Exception as e:
            print(f"é€‰æ‹©å™¨å»ºè®®å¤±è´¥: {e}")
            return {}
    
    def generate_css_selector(self, element) -> str:
        """ç”ŸæˆCSSé€‰æ‹©å™¨"""
        if element.get('id'):
            return f"#{element['id']}"
        elif element.get('class'):
            classes = ' '.join(element['class'])
            return f".{classes.replace(' ', '.')}"
        else:
            return element.name
    
    def suggest_article_selectors(self) -> Dict[str, str]:
        """å»ºè®®æ–‡ç« é¡µé¢é€‰æ‹©å™¨"""
        return {
            'title': 'h1, .title, .headline, [class*="title"]',
            'content': '.content, .article-content, .post-content, [class*="content"]',
            'author': '.author, .byline, [class*="author"]',
            'publish_time': '.time, .date, .publish-time, [class*="time"]',
            'summary': '.summary, .excerpt, .description, [class*="summary"]'
        }
    
    def suggest_pagination_selectors(self) -> Dict[str, str]:
        """å»ºè®®åˆ†é¡µé€‰æ‹©å™¨"""
        return {
            'next_page': '.next, .next-page, [class*="next"]',
            'page_numbers': '.page, .pagination a, [class*="page"]'
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ™ºèƒ½æ–°é—»æºå‘ç°å·¥å…·")
    print("=" * 60)
    
    # ç¤ºä¾‹ç½‘ç«™åˆ—è¡¨
    example_sites = [
        "https://news.sina.com.cn",
        "https://news.qq.com", 
        "https://news.163.com",
        "https://www.36kr.com",
        "https://www.ifanr.com"
    ]
    
    print("ğŸ“‹ ç¤ºä¾‹ç½‘ç«™:")
    for i, site in enumerate(example_sites, 1):
        print(f"   {i}. {site}")
    
    print("\nğŸ’¡ è¾“å…¥è¦åˆ†æçš„ç½‘ç«™URL (æˆ–æŒ‰Enterä½¿ç”¨ç¤ºä¾‹):")
    user_input = input("URL: ").strip()
    
    if not user_input:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªç¤ºä¾‹ç½‘ç«™
        target_url = example_sites[0]
        print(f"ğŸ¯ ä½¿ç”¨ç¤ºä¾‹ç½‘ç«™: {target_url}")
    else:
        target_url = user_input
    
    # å¼€å§‹æ™ºèƒ½å‘ç°
    discovery = SmartSourceDiscovery()
    discovered_sources = discovery.discover_news_sources(target_url)
    
    if discovered_sources:
        print(f"\nğŸ‰ å‘ç° {len(discovered_sources)} ä¸ªæ–°é—»æº:")
        print("=" * 60)
        
        for i, source in enumerate(discovered_sources, 1):
            print(f"\n{i}. {source['name']}")
            print(f"   ğŸŒ URL: {source['url']}")
            print(f"   ğŸ”§ ç±»å‹: {source['type']}")
            print(f"   âš™ï¸ è§£æå™¨: {source['parser']}")
            print(f"   â° çˆ¬å–é—´éš”: {source['crawl_interval']} ç§’")
            print(f"   ğŸ” å‘ç°æ–¹å¼: {source['discovery_method']}")
            
            if 'config' in source:
                print(f"   âš™ï¸ é…ç½®: {json.dumps(source['config'], indent=2, ensure_ascii=False)}")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. éªŒè¯å‘ç°çš„æ–°é—»æº")
        print("2. è°ƒæ•´çˆ¬è™«é…ç½®")
        print("3. æ·»åŠ åˆ°ç³»ç»Ÿä¸­")
        print("4. æµ‹è¯•çˆ¬è™«åŠŸèƒ½")
        
        # ä¿å­˜å‘ç°ç»“æœ
        with open('discovered_sources.json', 'w', encoding='utf-8') as f:
            json.dump(discovered_sources, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å‘ç°ç»“æœå·²ä¿å­˜åˆ°: discovered_sources.json")
        
    else:
        print("\nâŒ æœªå‘ç°æ–°é—»æºï¼Œå¯èƒ½åŸå› :")
        print("1. ç½‘ç«™ä¸æ˜¯æ–°é—»ç½‘ç«™")
        print("2. ç½‘ç«™ç»“æ„ç‰¹æ®Š")
        print("3. éœ€è¦æ‰‹åŠ¨åˆ†æ")

if __name__ == "__main__":
    main()
